---
title: "Why your code doesn't reproduce: Lessons learned from Meta-Psychology"
title-slide-style: pandoc
author: "Lucija Batinovic & Rickard Carlsson<br>Meta-Psychology<br>March 10, 2023"
format:
  revealjs:
    slide-number: c/t
    footer: "[Meta-Psychology](https://open.lnu.se/index.php/metapsychology/index)"
    title-slide-attributes: 
      data-background-image: "images/logo.png"
      data-background-size: 17%
      data-background-position: 1.5% 1.5%
    theme: [custom.scss]
    execute:
      echo: true
      eval: false
   #incremental: true
    scrollable: true
   #smaller: true
---

## Agenda
 
-   Meta-Psychology computational reproducibility assessment
-   Common Mistakes in Computational Reproducibility
-   Best Practices for Reproducible Analyses
-   Exercise (1 hour)
-   Discussion (15 min)

## Quick overview of the exercise
- try to reproduce code from a paper accepted for publication in MP
- note down what's causing troubles in reproducing the paper
- note down what is correctly done
- compare the results from the code output to reported results in the manuscript
## Introduction to reproducibility

## Transparency is a way out of the replication crisis
- Yet, transparency only allows assessing reliability of research findings
- Much focus has been given to replication (i.e., same analysis on new data) or robustness (i.e., new analyses same data)
- Less attention to the most basic:
    - keeping data, analyses and reporting free from errors
- Computational reproducibility still rare

## Meta-Psychology

- Meta-Psychology pioneering in this area (#1 on TOP FACTOR)
- Meta-Psychology is a community run journal
- No APC (Diamond OA)
- Publishes anything “meta” in psychology:
    - reviews, meta-analyses, commentaries, replications.
- Educational psychology is, of course an important area
- This talk is our 5 years experience with computational reproducibility at MP


# Computational Reproducibility Check 
## Overview {.smaller}
- 47 articles reproduced since journal's inception in 2017
- papers reproduced in 2022: 13
    - papers reproduced without any errors: 2 (with one being simulation study that required running code with fewer iterations)
    - errors in code: 8 (e.g., missing library calls, problems knitting reports, container-emulating packages, deprecated functions)
    - errors in reported results: 6 (e.g., wrong sample size, mean, plot output discrepancy, copy paste error, package update changed output)

## Reproducibility Checklist
![](images/checklist.png){fig-align="center"}

# Common Errors in Computational Reproducibility
## Most common errors we encountered happened because of:
- Renaming files
- Functions that were not working as they should
    - Deprecated functions
- Hard-coding file paths
- Older software versions

##  Functions Not Working {.smaller}
Common reasons why a function may not run properly: \par
- Incorrect input arguments or parameters
- Undefined or out-of-scope variables
- Missing or outdated dependencies
```{r}
# Example function call without loading library
my_function()

`Error in my_function() : could not find function "my_function"`
```
- Deprecated functions

```{r}
mydata <- read.csv("myfile.csv", header=TRUE)

`Warning message: In read.csv("myfile.csv", header = TRUE) :
   'read.csv' is deprecated: use 'read.csv2' instead`
```

- Improper syntax or formatting errors
- Logical errors or incorrect algorithmic design

<!-- ## How to troubleshoot function errors -->
<!-- - Demonstrate how to troubleshoot function errors in R -->

<!-- ```{r} -->
<!-- # Using the help() function -->
<!-- help(lm) -->
<!-- # Using the ? shortcut -->
<!-- ?lm -->
<!-- ``` -->

## Hard-Coding File Paths
- hard-coding the absolute path to the file will cause issues when running the code in a different environment
```{r}
# Hard-coded file path
data_file <- "C:/Users/username/Documents/data.csv"

# Load data from file
data <- read.csv(data_file)
```

- relative paths allow reproducing code across computers and operating systems
```{r}
# load data from a directory in the project 
data_file <- "data/data.csv"
```

## Software Versions
- Not stating which version of the software was used can cause troubles while reproducing the results in different environments
```{r}
Error: package 'dplyr' requires R version 4.0.0 or later
```

- on the other hand, some packages may stop being maintained and newer versions of software can cause issues with functions

```{r}
Warning: package 'plyr' was built under R version 3.6.3
```


# Reasons for non-reproducible Results
## Most common reasons for non-reproducible results
- copy-paste errors
- wrong rounding
- simulation based analyses 

## Setting Seed for Simulations
- bootstrapping confidence intervals
- simulating datasets
```{r}
#| echo = TRUE, eval = TRUE
head(rnorm(100, mean = 0, sd = 1))
```

```{r}
#| echo = TRUE, eval = TRUE
set.seed(1234)
head(rnorm(100, mean = 0, sd = 1))
```


## Wrong Rounding
```{r}
#| echo = TRUE, eval = TRUE
data <- iris
mean(iris$Sepal.Width)
round(mean(iris$Sepal.Width))
round(mean(iris$Sepal.Width), 2)
```


## Copy-Paste Errors

```{r}
#| echo = TRUE, eval = TRUE
data <- iris
# Hard-coded result
cat("The mean value is 5.")

# Extracted from code
cat(paste0("The mean value is ", round(mean(iris$Sepal.Length), 2), "."))

```

- compiling reports that contain the code and results in one document allows a simpler workflow and minimizes errors caused by copy-pasting
- [papaja](https://cran.r-project.org/web/packages/papaja/index.html) package in R

<!-- Examples of how to do it properly starts here -->

# Project structure {.smaller}
## Importance of structure
- [Psych-DS](https://psych-ds.github.io/)
- creating projects allows using relative paths 
- projects can be easily sent to and managed by other people
- hierarchical project structure makes the analysis process clearer for the third party

```{r}
#| eval = FALSE
my_project/
├── README.txt
├── codebook.txt
├── data/
│   ├── raw/
│   │   ├── data_file_1.csv
│   │   ├── data_file_2.csv
│   │   └── ...
│   ├── processed/
│   │   ├── data_file_1_cleaned.csv
│   │   ├── data_file_2_cleaned.csv
│   │   └── ...
│   └── ...
├── docs/
│   ├── report.pdf
│   ├── manuscript.Rmd
│   └── ...
├── src/
│   ├── data_cleaning.R
│   ├── data_analysis.R
│   └── ...
└── ...

```

## Best practices for file naming conventions
- consistency: keep capitalization consistent, as well as use of punctuation, avoid special characters
- clarity: names should be descriptive and clear, even if it means it will be longer
- chronology: names should indicate the correct order of steps

## README and codebook files {.smaller}
:::: {.columns}

::: {.column width="50%"}
::: {.fragment}

README

::: {.incremental}
- Computing environment; package version and time necessary to compute (show reproducibility checklists) 
- Project description: A brief description of the project and its purpose.
- Installation instructions: Step-by-step instructions on how to install and set up the project. This should include any dependencies or prerequisites required to run the project
:::

:::

:::

::: {.column width="50%"}
::: {.fragment}

Codebook

::: {.incremental}
- Overview of all variables: name and description of each variable
- Description
:::

:::

::: 

::::

## Containerization {.smaller}
- [Docker](https://www.docker.com/)
- [Codeocean](https://codeocean.com/) 
- R packages (e.g., [renv](https://cran.r-project.org/web/packages/renv/index.html))
![](images/codeocean.png){fig-align="center"}

## GUI alternative {.smaller}
- open source: [JASP](https://jasp-stats.org/) or [JAMOVI](https://www.jamovi.org/)
- jamovi files can be saved with analyses and edited data in a transparent way
*Note: although .omv (jamovi) files include datasets, always provide raw .tsv/.csv files separately
![](images/jamovi-lr.png){fig-align="center"}



# Practice

## Task
- try to reproduce code from a paper accepted for publication in MP
- note down what's causing troubles in reproducing the paper (if anything)
- note down what is correctly done
- compare the results from the code output to reported results in the manuscript

